---
title: "Amazon Product Analysis"
authors: 'Nathan Thomas, Benjamin Tuohey, Ivan Voinov'
output: html_notebook
---



# Importing/Cleaning Data
```{r}
Amazon_Products <- read.csv('Amazon_Products.csv')
head(Amazon_Products)
```

## Cleaning Data
```{r}
colnames(Amazon_Products)
```

### Removes 800+ empty columns - filters down to 14
```{r}
Amazon_Products_Cleaned <- subset(Amazon_Products, select = c(uniq_id, product_name, manufacturer, price, number_available_in_stock, number_of_reviews, number_of_answered_questions, average_review_rating, amazon_category_and_sub_category, description, product_information, product_description, items_customers_buy_after_viewing_this_item, customer_questions_and_answers))
```

```{r}
head(Amazon_Products_Cleaned)
```

### Writes Dataset as CSV for export
```{r}
write.csv(Amazon_Products_Cleaned,'Amazon_Products_Cleaned.csv')
```

### Convert Price from Pound to Dollar and variable transformation

Price is currently character, need to remove the pound symbol and convert to float type
```{r}
Amazon_Products_Cleaned$price<-gsub("Â£","",as.character(Amazon_Products_Cleaned$price))
```

```{r}
library(tidyverse)
Amazon_Products_Cleaned <- Amazon_Products_Cleaned %>%
  mutate(price = as.numeric(as.character(Amazon_Products_Cleaned$price)), 
         number_of_reviews = as.numeric(number_of_reviews),
         number_of_answered_questions = as.numeric(number_of_answered_questions))
Amazon_Products_Cleaned$price <- Amazon_Products_Cleaned$price*1.30
```

### Convert review rating to number

```{r}
Amazon_Products_Cleaned$average_review_rating <- substr(Amazon_Products_Cleaned$average_review_rating,1,3)
Amazon_Products_Cleaned <- Amazon_Products_Cleaned %>%
  mutate(average_review_rating = as.numeric(average_review_rating))
  
```


# Initial Statistics/Plots
## Statistics

```{r}
Amazon_Products_Cleaned %>%
  summarise(num_cases = n(), 
            mean_price = mean(price, na.rm=T),
            med_price = median(price, na.rm=T),
            mean_reviews = mean(number_of_reviews, na.rm=T),
            mean_rating = mean(average_review_rating, na.rm=T)
            )
```

## Manufacturer Analysis

```{r}
Dis_Manufacturer <- Amazon_Products_Cleaned %>% 
  group_by(manufacturer) %>%
  filter(n() >= 1)
```

Here we see there are 2654 unique manufacturers in our data. 
```{r}
n_distinct(Dis_Manufacturer$manufacturer)
```


For our analysis, we consider big manufacturer those that have 50 or more products listed.
```{r}
Big_Manufacturer <- Amazon_Products_Cleaned %>% 
  group_by(manufacturer) %>%
  filter(n() >= 50)
```

We observe that there are 24 manufacturers that have 50 or more product listings.
```{r}
n_distinct(Big_Manufacturer$manufacturer)
```
Manufacturer Distribution
```{r}
counts <- table(Big_Manufacturer$manufacturer)
barplot(counts, las = 2,
        cex.names = 0.7,main="Manufacturer Distribution", ylab = "Number of Product Listings")
```

Sorted Distribution
```{r}
barplot(sort(counts, decreasing = TRUE), las = 2,
        cex.names = 0.7,main="Manufacturer Distribution", ylab = "Number of Product Listings")
```


## Plots
###Boxplot
```{r}
Amazon_Products_Cleaned %>%
  ggplot(aes(y=price)) +
  geom_boxplot()
```
There are a lot of outliers, let's adjust the scale.

```{r}
Amazon_Products_Cleaned %>%
  ggplot(aes(y=price)) +
  geom_boxplot() +
  ylim(0,25)
```
Density Plot of Price Distribution (Skewed to the right)
```{r}
ggplot(data = Amazon_Products_Cleaned, aes(x = price)) + geom_density(fill = 'cyan')
```



# Clustering
### Looking for groups

```{r}
# pairs can only look at numeric variables
Amazon_Products_Cleaned_numeric <- Amazon_Products_Cleaned %>%
  select(price, number_of_reviews, number_of_answered_questions, average_review_rating)
pairs(Amazon_Products_Cleaned_numeric)
```
```{r}
eu_dist<- dist(Amazon_Products_Cleaned_numeric, method = "euclidean", upper = TRUE )
image(as.matrix(eu_dist), main = "Euclidean distance")

```


```{r}
# Creating a hierarchy of data trying different linkage methods 
library(factoextra)
hc_single <- hclust(eu_dist, method = "single") #for single linkage
hc_complete <- hclust(eu_dist, "complete") #for complete linkage 
hc_average <- hclust(eu_dist, method = "average") # for average linkage
hc_centroid <- hclust(eu_dist, method = "centroid") #for centroid linkage
```

```{r}
#Visualizing by creating dendrogram for each
plot(hc_single)
plot(hc_complete)
plot(hc_average)
plot(hc_centroid)
```

```{r}
#
fviz_dend(hc_complete)
```

```{r}
#performing pca on data
res <- Amazon_Products_Cleaned_numeric %>% prcomp(scale = TRUE)
res
```

```{r}
# how much variance is explained by each pc 
get_eig(res)
```

```{r}
# take out missing values and find optimal number of clusters using k means
Amazon_numeric_nomiss<- na.omit(Amazon_Products_Cleaned_numeric)
Amazon_numeric_nomiss<- scale(Amazon_numeric_nomiss)
set.seed(1234)
fviz_nbclust(Amazon_numeric_nomiss, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")

```

```{r}

set.seed(123)
fviz_nbclust(Amazon_numeric_nomiss, kmeans, method = "wss")

```


```{r}
km<- kmeans(Amazon_numeric_nomiss, centers = 3, nstart = 25)
print(km)
```

```{r}
#plot results of k means model
fviz_cluster(km, data = Amazon_numeric_nomiss)
```

```{r}
res<- Amazon_numeric_nomiss %>% kmeans(3)
str(res)
```

```{r}
#silhouette widths
library(cluster)
distance<- get_dist(Amazon_numeric_nomiss, method = 'euclidean')
sil<- silhouette(x= res$cluster, dist= distance)
summary(sil)
```

```{r}
sil %>% head()
```

```{r}
fviz_silhouette(sil)
```
```{r}
#gap statistic applied on kmeans clustering
library(NbClust)
NbClust(Amazon_numeric_nomiss,  distance = "euclidean", method = "kmeans", index='gap')
```



# Linear Regression
## Validation setup
```{r}

set.seed(380)
Amazon_Products_Cleaned_numeric <- Amazon_Products_Cleaned_numeric %>%
  filter(is.na(price) == F, is.na(average_review_rating) == F)

train_obs <- createDataPartition(Amazon_Products_Cleaned_numeric$price, , p = 0.5, list = FALSE)

# generating training dataset from the train_obs
training_df <- Amazon_Products_Cleaned_numeric[train_obs, ]
training_df

testing_df  <- Amazon_Products_Cleaned_numeric[-train_obs, ]
testing_df
```
Here we split the data into only the numeric variables and then into testing and training sets. We decided to go with the validation set approach because of it's simplicity to implement and the dataset is relatively large so the LOOCV approach would be computationally expensive. Here we also removed observations that have NA values that would be problems later in the analysis.

## Feature Selection

```{r}
library(leaps)
regfit_full = regsubsets(price ~ ., data = training_df,  nvmax = 4, method="exhaustive")
summary(regfit_full)
```
From this summary we can see that the average review rating is the most important feature followed by number of answered questions and then number of reviews.

```{r}
reg_summary <- summary(regfit_full) #get the summary

par(mfrow=c(2,2))
#rss plot -  NOT USEFUL
plot(reg_summary$rss ,xlab="Number of Variables ",ylab="RSS",type="l")

#adjr2 plot
plot(reg_summary$adjr2 ,xlab="Number of Variables ", ylab="Adjusted RSq",type="l")
max_adjr2 <- which.max(reg_summary$adjr2)
points(max_adjr2,reg_summary$adjr2[max_adjr2], col="red",cex=2,pch=20)

# AIC criterion (Cp) to minimize
plot(reg_summary$cp ,xlab="Number of Variables ",ylab="Cp", type='l')
min_cp <- which.min(reg_summary$cp )
points(min_cp, reg_summary$cp[min_cp],col="red",cex=2,pch=20)

# BIC criterion to minimize
plot(reg_summary$bic ,xlab="Number of Variables ",ylab="BIC",type='l')
min_bic <- which.min(reg_summary$bic)
points(min_bic,reg_summary$bic[min_bic],col="red",cex=2,pch=20)
```
Here are four plots with different criteria for how many predictors is best. Using Cp and Adjusted R-Squared criteria the full model is the best while a simple model is best according to BIC criteria. Knowing this we will first consider two models, one with all the predictors and one with just the number of reviews. One thing of note with these plots is that the values for RSS are extremely high and the value for adjusted R-Squared is extremely low. This will be discussed more in the conclusion of this section.

## Model Analysis
### Summaries
```{r}
model_full <- lm(price ~ ., data = training_df)
summary(model_full)
```
Here is the summary of the full model using all predictors. With p-values of 0.82 and 0.11 the intercept and number of reviews are not significant to the model when added last, respectively. The other two predictors are significant with a 0.05 significance level. 

```{r}
simp_model <- lm(price~average_review_rating, data = training_df)
summary(simp_model)
```
Here is the simple model with just average review rating. Like the full model the intercept is not significant with a p-value of 0.86. Knowing this, let's try a simple model without the intercept.

```{r}
simp_model_noint <- lm(price~0+average_review_rating, data = training_df)
summary(simp_model_noint)
```
This model gives us a much better R-squared value while having similar values for error. With that in mind, this is the model that will be used going forward.

### Diagnostic Plots

```{r}
par (mfrow = c(2, 2))
plot (simp_model_noint)
```

From these diagnostic plots we can see that even the best model has many assumption violations. For example, the Normal QQ plot shows that the residuals are not normally distributed because they do not follow a straight line. It has also identified some observations that could be outliers.

## Cross-validation

```{r}
predictions <- predict(simp_model_noint, testing_df)
```

```{r}
data.frame( R2 = R2(predictions, testing_df$price, na.rm=T),
            RMSE = RMSE(predictions, testing_df$price, na.rm=T),
            MSE = RMSE(predictions, testing_df$price, na.rm=T)^2,
            MAE = MAE(predictions, testing_df$price, na.rm=T))
```
Here the values for error are relatively high and R-squared is low. This shows that even one of the best models is not such a good fit. Let's try adding some higher order terms.

```{r}
model1 <- lm(data = training_df, price ~ poly(average_review_rating,2))
summary(model1)

```

```{r}
# power 3
model2 <- lm(data = training_df, price ~ poly(average_review_rating,3))
summary(model2)
```
Here we can see that the quadratic model is significant while the cubic model is not. Let's compare the simple model and the quadratic model.

```{r}
predictions1 <- predict(model1, testing_df)

```

```{r}
rbind(data.frame( R2 = R2(predictions, testing_df$price, na.rm=T),
            RMSE = RMSE(predictions, testing_df$price, na.rm=T),
            MSE = RMSE(predictions, testing_df$price, na.rm=T)^2,
            MAE = MAE(predictions, testing_df$price, na.rm=T)),
data.frame( R2 = R2(predictions1, testing_df$price, na.rm=T),
            RMSE = RMSE(predictions1, testing_df$price, na.rm=T),
            MSE = RMSE(predictions1, testing_df$price, na.rm=T)^2,
            MAE = MAE(predictions1, testing_df$price, na.rm=T)))
```
The quadratic model is better in almost all respects, but not by much.

## Conclusions
With everything in mind from the previous sections, we conclude that linear regression is not a good fit for this dataset. Even the best models have very low values for R-squared and high values for error, and testing error is very high. No matter what model is used it would not be a good fit for making predictions about the future.
